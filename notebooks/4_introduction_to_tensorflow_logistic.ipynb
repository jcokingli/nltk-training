{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4-introduction-to-tensorflow-logistic.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "7NIxjzpH1FQ_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ffT8D0Id3-WL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8kbm084S4EEY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = load_iris().data\n",
        "y = to_categorical(load_iris().target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N80Ew4GB4NIZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "c1abb908-667a-425f-c2df-bef5c7625ea0"
      },
      "cell_type": "code",
      "source": [
        "x[:10], y[:10], x.shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[5.1, 3.5, 1.4, 0.2],\n",
              "        [4.9, 3. , 1.4, 0.2],\n",
              "        [4.7, 3.2, 1.3, 0.2],\n",
              "        [4.6, 3.1, 1.5, 0.2],\n",
              "        [5. , 3.6, 1.4, 0.2],\n",
              "        [5.4, 3.9, 1.7, 0.4],\n",
              "        [4.6, 3.4, 1.4, 0.3],\n",
              "        [5. , 3.4, 1.5, 0.2],\n",
              "        [4.4, 2.9, 1.4, 0.2],\n",
              "        [4.9, 3.1, 1.5, 0.1]]), array([[1., 0., 0.],\n",
              "        [1., 0., 0.],\n",
              "        [1., 0., 0.],\n",
              "        [1., 0., 0.],\n",
              "        [1., 0., 0.],\n",
              "        [1., 0., 0.],\n",
              "        [1., 0., 0.],\n",
              "        [1., 0., 0.],\n",
              "        [1., 0., 0.],\n",
              "        [1., 0., 0.]], dtype=float32), (150, 4))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "lL95w_j14Ro4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input_features = tf.placeholder(dtype=tf.float32,\n",
        "                                shape = [None, x.shape[1]])\n",
        "input_labels = tf.placeholder(dtype=tf.float32,\n",
        "                             shape = [None, 3])\n",
        "\n",
        "weights = tf.Variable(tf.random_normal(shape=[x.shape[1], 3]))\n",
        "biases = tf.Variable(tf.random_normal(shape=[3]))\n",
        "#linear_model = tf.add(tf.matmul(input_features,weights), biases)\n",
        "logistic_model = tf.nn.softmax(tf.matmul(input_features, weights) + biases) # Softmax\n",
        "\n",
        "loss = tf.reduce_mean(-tf.reduce_sum(input_labels*tf.log(logistic_model), reduction_indices=1))\n",
        "train_op = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SK4TKYel7bf1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6fcUIZR08e5o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def next_batch(batch_size, features, labels):\n",
        "  indices = np.arange(start=0, stop=features.shape[0])\n",
        "  np.random.shuffle(indices)\n",
        "  indices = indices[:batch_size]\n",
        "  return features[indices], labels[indices]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sjNVHiEt9Q6Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1777
        },
        "outputId": "97cdee50-13ad-4c07-81aa-45a523239ab3"
      },
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "for epoch in range(100):\n",
        "  for index in range(int(x.shape[0] / batch_size)):\n",
        "    mini_batch_x, mini_batch_y = next_batch(batch_size = batch_size, features =x, labels=y)\n",
        "    \n",
        "    _, train_loss = sess.run([train_op, loss], feed_dict={\n",
        "        input_features: mini_batch_x,    \n",
        "        input_labels: mini_batch_y\n",
        "    })\n",
        "\n",
        "  print('Epoch {}, loss: {}'.format(epoch, train_loss))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, loss: 3.3668956756591797\n",
            "Epoch 1, loss: 3.188976764678955\n",
            "Epoch 2, loss: 3.903873920440674\n",
            "Epoch 3, loss: 2.563350200653076\n",
            "Epoch 4, loss: 1.9455348253250122\n",
            "Epoch 5, loss: 3.381093978881836\n",
            "Epoch 6, loss: 1.9276912212371826\n",
            "Epoch 7, loss: 2.1840262413024902\n",
            "Epoch 8, loss: 1.8095295429229736\n",
            "Epoch 9, loss: 2.625025749206543\n",
            "Epoch 10, loss: 2.370377779006958\n",
            "Epoch 11, loss: 1.8289167881011963\n",
            "Epoch 12, loss: 1.7707431316375732\n",
            "Epoch 13, loss: 1.3646618127822876\n",
            "Epoch 14, loss: 1.4216926097869873\n",
            "Epoch 15, loss: 1.418808937072754\n",
            "Epoch 16, loss: 0.9460754990577698\n",
            "Epoch 17, loss: 1.1326403617858887\n",
            "Epoch 18, loss: 0.8647690415382385\n",
            "Epoch 19, loss: 1.1687759160995483\n",
            "Epoch 20, loss: 1.0594818592071533\n",
            "Epoch 21, loss: 0.5575945377349854\n",
            "Epoch 22, loss: 0.9379303455352783\n",
            "Epoch 23, loss: 0.6068971157073975\n",
            "Epoch 24, loss: 0.7939640283584595\n",
            "Epoch 25, loss: 0.5305883884429932\n",
            "Epoch 26, loss: 0.6892468929290771\n",
            "Epoch 27, loss: 0.5624935626983643\n",
            "Epoch 28, loss: 0.8056604862213135\n",
            "Epoch 29, loss: 0.7305493354797363\n",
            "Epoch 30, loss: 0.7134623527526855\n",
            "Epoch 31, loss: 0.7503851652145386\n",
            "Epoch 32, loss: 0.6220951676368713\n",
            "Epoch 33, loss: 0.7635900974273682\n",
            "Epoch 34, loss: 0.8173688650131226\n",
            "Epoch 35, loss: 0.724190354347229\n",
            "Epoch 36, loss: 0.8161309361457825\n",
            "Epoch 37, loss: 0.6774619817733765\n",
            "Epoch 38, loss: 0.923648476600647\n",
            "Epoch 39, loss: 0.7038272023200989\n",
            "Epoch 40, loss: 0.6402943134307861\n",
            "Epoch 41, loss: 0.6510002613067627\n",
            "Epoch 42, loss: 0.787318229675293\n",
            "Epoch 43, loss: 0.8454157710075378\n",
            "Epoch 44, loss: 0.6452686786651611\n",
            "Epoch 45, loss: 0.4713752269744873\n",
            "Epoch 46, loss: 0.6740731000900269\n",
            "Epoch 47, loss: 0.7003326416015625\n",
            "Epoch 48, loss: 0.7457205653190613\n",
            "Epoch 49, loss: 0.5966725945472717\n",
            "Epoch 50, loss: 0.6398975849151611\n",
            "Epoch 51, loss: 0.6949335336685181\n",
            "Epoch 52, loss: 0.5164585709571838\n",
            "Epoch 53, loss: 0.6839063167572021\n",
            "Epoch 54, loss: 0.657734215259552\n",
            "Epoch 55, loss: 0.5737675428390503\n",
            "Epoch 56, loss: 0.6737570762634277\n",
            "Epoch 57, loss: 0.6856306791305542\n",
            "Epoch 58, loss: 0.7780926823616028\n",
            "Epoch 59, loss: 0.672191858291626\n",
            "Epoch 60, loss: 0.7578449845314026\n",
            "Epoch 61, loss: 0.800597071647644\n",
            "Epoch 62, loss: 0.5144765377044678\n",
            "Epoch 63, loss: 0.5652283430099487\n",
            "Epoch 64, loss: 0.5823990106582642\n",
            "Epoch 65, loss: 0.6500786542892456\n",
            "Epoch 66, loss: 0.6052836775779724\n",
            "Epoch 67, loss: 0.5761408805847168\n",
            "Epoch 68, loss: 0.5350077152252197\n",
            "Epoch 69, loss: 0.6612746715545654\n",
            "Epoch 70, loss: 0.6402165293693542\n",
            "Epoch 71, loss: 0.6257179379463196\n",
            "Epoch 72, loss: 0.6652901768684387\n",
            "Epoch 73, loss: 0.5367420315742493\n",
            "Epoch 74, loss: 0.7969958186149597\n",
            "Epoch 75, loss: 0.4991868734359741\n",
            "Epoch 76, loss: 0.6237745881080627\n",
            "Epoch 77, loss: 0.574806809425354\n",
            "Epoch 78, loss: 0.6679410934448242\n",
            "Epoch 79, loss: 0.5606303215026855\n",
            "Epoch 80, loss: 0.5851370096206665\n",
            "Epoch 81, loss: 0.567965030670166\n",
            "Epoch 82, loss: 0.5615890026092529\n",
            "Epoch 83, loss: 0.6260964870452881\n",
            "Epoch 84, loss: 0.682915449142456\n",
            "Epoch 85, loss: 0.5853537321090698\n",
            "Epoch 86, loss: 0.5953496098518372\n",
            "Epoch 87, loss: 0.8220178484916687\n",
            "Epoch 88, loss: 0.6695904731750488\n",
            "Epoch 89, loss: 0.7829416990280151\n",
            "Epoch 90, loss: 0.7906661629676819\n",
            "Epoch 91, loss: 0.6992661952972412\n",
            "Epoch 92, loss: 0.6102826595306396\n",
            "Epoch 93, loss: 0.6055763363838196\n",
            "Epoch 94, loss: 0.5452700853347778\n",
            "Epoch 95, loss: 0.6502364873886108\n",
            "Epoch 96, loss: 0.7289144992828369\n",
            "Epoch 97, loss: 0.556261420249939\n",
            "Epoch 98, loss: 0.4643884599208832\n",
            "Epoch 99, loss: 0.7274868488311768\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}